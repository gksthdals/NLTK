{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gksthdals/NLTK/blob/main/07.%20Extracting_Information_from_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT6zdDWtuKfk"
      },
      "outputs": [],
      "source": [
        "# How can we build a system that extracts structured data, such as tables, from unstructured text?\n",
        "# What are some robust methods for identifying the entities and relationships described in a text?\n",
        "# Which corpora are appropriate for this work, and how do we use them for training and evaluating our models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1wMMeQwuwiO"
      },
      "source": [
        "## 1. Information Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJMw3ZsvGYV"
      },
      "outputs": [],
      "source": [
        "locs = [('Omnicom', 'IN', 'New York'),\n",
        "        ('DDB Needham', 'IN', 'New York'),\n",
        "        ('Kaplan Thaler Group', 'IN', 'New York'),\n",
        "        ('BBDO South', 'IN', 'Atlanta'),\n",
        "        ('Georgia-Pacific', 'IN', 'Atlanta')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx-ZtSS4vdI-",
        "outputId": "e8782efe-64fd-4763-a0d9-3064d3a1ae39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['BBDO South', 'Georgia-Pacific']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = [e1 for (e1, rel, e2) in locs if e2 == 'Atlanta']\n",
        "query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ1oJlDEwTtd"
      },
      "source": [
        "### Information Extraction Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INFN4hgNwrQJ"
      },
      "outputs": [],
      "source": [
        "# 1. sentence segmentation\n",
        "# 2. tokenization\n",
        "# 3. part of speech tagging\n",
        "# 4. entity detection\n",
        "# 5. relation detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avjp3RfyxDv5",
        "outputId": "44d5dc57-116d-4ef3-be3c-389900e31be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk, re, pprint\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHT2w09vw_QG"
      },
      "outputs": [],
      "source": [
        "def ie_preprocess(document):\n",
        "  sentences = nltk.sent_tokenize(document)\n",
        "  sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "  sentences = [nltk.pos_tag(sent) for sent in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBn-oESNx7Kw"
      },
      "source": [
        "## 2. Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOnc0ph2ySSa"
      },
      "source": [
        "### Noun Phrase Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLBEB65cy66u",
        "outputId": "95b2e3c1-bb36-4dd8-fff9-6f749dfe4ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
            "  barked/VBD\n",
            "  at/IN\n",
            "  (NP the/DT cat/NN))\n"
          ]
        }
      ],
      "source": [
        "sentence = [('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN')]\n",
        "\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(sentence)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfBrK3lJ4Wp5"
      },
      "source": [
        "### Tag Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqNuwRIs4z_j"
      },
      "outputs": [],
      "source": [
        "# <DT>?<JJ.*>*<NN.*>+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzIy7z4Y43VJ"
      },
      "source": [
        "### Chunking with Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9K_u0v-5F8a"
      },
      "outputs": [],
      "source": [
        "grammar = r\"\"\"\n",
        "  NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and noun\n",
        "      {<NNP>+}              # chunk sequences of proper nouns\n",
        "\"\"\"\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
        "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B10J5z55ic9",
        "outputId": "2a566a52-2fc0-4673-ef15-d4373f5a9bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP Rapunzel/NNP)\n",
            "  let/VBD\n",
            "  down/RP\n",
            "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
          ]
        }
      ],
      "source": [
        "print(cp.parse(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW_zlm565kDA",
        "outputId": "e12c177a-9db1-45f3-9678-2979ba3e1785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S (NP money/NN market/NN) fund/NN)\n"
          ]
        }
      ],
      "source": [
        "nouns = [('money', 'NN'), ('market', 'NN'), ('fund', 'NN')]\n",
        "grammar = \"NP: {<NN><NN>} # Chunk two consecutive nouns\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "print(cp.parse(nouns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LbryCIc6nMJ",
        "outputId": "892aab2c-072d-439f-db15-bbb48187792a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoE2wFXA6Gi6",
        "outputId": "cc73afc0-6640-48d4-e516-833685ca57ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(CHUNK combined/VBN to/TO achieve/VB)\n",
            "(CHUNK continue/VB to/TO place/VB)\n",
            "(CHUNK serve/VB to/TO protect/VB)\n",
            "(CHUNK wanted/VBD to/TO wait/VB)\n",
            "(CHUNK allowed/VBN to/TO place/VB)\n",
            "(CHUNK expected/VBN to/TO become/VB)\n",
            "(CHUNK expected/VBN to/TO approve/VB)\n",
            "(CHUNK expected/VBN to/TO make/VB)\n",
            "(CHUNK intends/VBZ to/TO make/VB)\n",
            "(CHUNK seek/VB to/TO set/VB)\n",
            "(CHUNK like/VB to/TO see/VB)\n"
          ]
        }
      ],
      "source": [
        "cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
        "brown = nltk.corpus.brown\n",
        "for sent in brown.tagged_sents()[:100]:\n",
        "  tree = cp.parse(sent)\n",
        "  for subtree in tree.subtrees():\n",
        "    if subtree.label() == 'CHUNK': print(subtree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSVQKa847KTI"
      },
      "source": [
        "### Chinking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh8qdVvQ7j9J",
        "outputId": "49444357-3916-4691-8c33-e5fc8b4dc5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
            "  barked/VBD\n",
            "  at/IN\n",
            "  (NP the/DT cat/NN))\n"
          ]
        }
      ],
      "source": [
        "grammar = r\"\"\"\n",
        "  NP:\n",
        "    {<.*>+}       # Chunk everything\n",
        "    }<VBD|IN>+{   # Chink sequences of VBD and IN\n",
        "\"\"\"\n",
        "\n",
        "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
        "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "print(cp.parse(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRF84Ef49H5D"
      },
      "source": [
        "### Representing Chunks: Tags vs Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvrtTKJE9xjR"
      },
      "source": [
        "## 3. Developing and Evaluating Chunkers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuJUAhxwEBdM"
      },
      "source": [
        "### Reading IOB Format and the CoNLL 2000 Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGx8lpO6ETCq",
        "outputId": "75ac75b3-131b-467f-9abe-3871fbc6b581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP he/PRP)\n",
            "  accepted/VBD\n",
            "  (NP the/DT position/NN)\n",
            "  of/IN\n",
            "  (NP vice/NN chairman/NN)\n",
            "  of/IN\n",
            "  (NP Carlyle/NNP Group/NNP)\n",
            "  ,/,\n",
            "  (NP a/DT merchant/NN banking/NN concern/NN)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "he PRP B-NP\n",
        "accepted VBD B-VP\n",
        "the DT B-NP\n",
        "position NN I-NP\n",
        "of IN B-PP\n",
        "vice NN B-NP\n",
        "chairman NN I-NP\n",
        "of IN B-PP\n",
        "Carlyle NNP B-NP\n",
        "Group NNP I-NP\n",
        ", , O\n",
        "a DT B-NP\n",
        "merchant NN I-NP\n",
        "banking NN I-NP\n",
        "concern NN I-NP\n",
        ". . O\n",
        "\"\"\"\n",
        "print(nltk.chunk.conllstr2tree(text, chunk_types=['NP']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWXy8U9uGMxV",
        "outputId": "905f5d2f-99dd-4da3-8e41-cfa43b9489ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('conll2000')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnY4ne3nEzax",
        "outputId": "6c9b41f4-a308-419b-fe20-ad34c80ccd84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PP Over/IN)\n",
            "  (NP a/DT cup/NN)\n",
            "  (PP of/IN)\n",
            "  (NP coffee/NN)\n",
            "  ,/,\n",
            "  (NP Mr./NNP Stone/NNP)\n",
            "  (VP told/VBD)\n",
            "  (NP his/PRP$ story/NN)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import conll2000\n",
        "print(conll2000.chunked_sents('train.txt')[99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8qy-MJGGXVk",
        "outputId": "24a8ed3d-96a9-4335-ff50-151190bf4cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  Over/IN\n",
            "  (NP a/DT cup/NN)\n",
            "  of/IN\n",
            "  (NP coffee/NN)\n",
            "  ,/,\n",
            "  (NP Mr./NNP Stone/NNP)\n",
            "  told/VBD\n",
            "  (NP his/PRP$ story/NN)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbzaqPnWGfKj"
      },
      "source": [
        "### Simple Evaluation and Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsusEA_yGnwr",
        "outputId": "c4302e58-4f8e-4395-8022-6ab5961785b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  43.4%%\n",
            "    Precision:      0.0%%\n",
            "    Recall:         0.0%%\n",
            "    F-Measure:      0.0%%\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import conll2000\n",
        "cp = nltk.RegexpParser(\"\")\n",
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "print(cp.evaluate(test_sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w1X8XzBGzDz",
        "outputId": "651ccaf5-7fe2-4fa0-8833-dd6edcce0af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  87.7%%\n",
            "    Precision:     70.6%%\n",
            "    Recall:        67.8%%\n",
            "    F-Measure:     69.2%%\n"
          ]
        }
      ],
      "source": [
        "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "print(cp.evaluate(test_sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8Jg1XfRHOYE"
      },
      "outputs": [],
      "source": [
        "class UnigramChunker(nltk.ChunkParserI):\n",
        "  def __init__(self, train_sents):\n",
        "    train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\n",
        "                  for sent in train_sents]\n",
        "    self.tagger = nltk.UnigramTagger(train_data)\n",
        "\n",
        "  def parse(self, sentence):\n",
        "    pos_tags = [pos for (word, pos) in sentence]\n",
        "    tagged_pos_tags = self.tagger.tag(pos_tags)\n",
        "    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
        "    conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
        "                 in zip(sentence, chunktags)]\n",
        "    return nltk.chunk.conlltags2tree(conlltags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI23JWNxITSR",
        "outputId": "9fa1c5f7-07bf-44f3-9691-109c121bf442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  92.9%%\n",
            "    Precision:     79.9%%\n",
            "    Recall:        86.8%%\n",
            "    F-Measure:     83.2%%\n"
          ]
        }
      ],
      "source": [
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
        "unigram_chunker = UnigramChunker(train_sents)\n",
        "\n",
        "print(unigram_chunker.evaluate(test_sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shgw60qpJmQ7",
        "outputId": "3f0770ac-a990-41e7-8535-61fe2f654079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'), (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'), ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'), ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'), ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'), ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'), ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'), ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n"
          ]
        }
      ],
      "source": [
        "postags = sorted(set(pos for sent in train_sents\n",
        "                         for (word, pos) in sent.leaves()))\n",
        "\n",
        "print(unigram_chunker.tagger.tag(postags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-V0X8JCLOFL"
      },
      "outputs": [],
      "source": [
        "class BigramChunker(nltk.ChunkParserI):\n",
        "  def __init__(self, train_sents):\n",
        "    train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\n",
        "                  for sent in train_sents]\n",
        "    self.tagger = nltk.BigramTagger(train_data)\n",
        "\n",
        "  def parse(self, sentence):\n",
        "    pos_tags = [pos for (word, pos) in sentence]\n",
        "    tagged_pos_tags = self.tagger.tag(pos_tags)\n",
        "    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
        "    conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
        "                 in zip(sentence, chunktags)]\n",
        "    return nltk.chunk.conlltags2tree(conlltags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJDvoMJoLy3z",
        "outputId": "18f386a7-cd1b-4827-c07c-01edddfac620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  93.3%%\n",
            "    Precision:     82.3%%\n",
            "    Recall:        86.8%%\n",
            "    F-Measure:     84.5%%\n"
          ]
        }
      ],
      "source": [
        "bigram_chunker = BigramChunker(train_sents)\n",
        "print(bigram_chunker.evaluate(test_sents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur656xiIMDT8"
      },
      "source": [
        "### Training Classifier-Based Chunkers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgTKWC4wMxkb"
      },
      "outputs": [],
      "source": [
        "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
        "\n",
        "  def __init__(self, train_sents):\n",
        "    train_set = []\n",
        "    for tagged_sent in train_sents:\n",
        "      untagged_sent = nltk.tag.untag(tagged_sent)\n",
        "      history = []\n",
        "      for i, (word, tag) in enumerate(tagged_sent):\n",
        "        featureset = npchunk_features(untagged_sent, i, history)\n",
        "        train_set.append( (featureset, tag) )\n",
        "        history.append(tag)\n",
        "    self.classifier = nltk.MaxentClassifier.train(\n",
        "        train_set, algorithm='megam', trace=0)\n",
        "\n",
        "  def tag(self, sentence):\n",
        "    history = []\n",
        "    for i, word in enumerate(sentence):\n",
        "      featureset = npchunk_features(sentence, i, history)\n",
        "      tag = self.classifier.classify(featureset)\n",
        "      history.append(tag)\n",
        "    return zip(sentence, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJSTRrpyN3y0"
      },
      "outputs": [],
      "source": [
        "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
        "  def __init__(self, train_sents):\n",
        "    tagged_sents = [[((w, t), c) for (w, t, c) in\n",
        "                     nltk.chunk.tree2conlltags(sent)]\n",
        "                    for sent in train_sents]\n",
        "    self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
        "\n",
        "  def parse(self, sentence):\n",
        "    tagged_sents = self.tagger.tag(sentence)\n",
        "    conlltags = [(w, t, c) for ((w, t), c) in tagged_sents]\n",
        "    return nltk.chunk.conlltags2tree(conlltags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTN0fxCROdc_"
      },
      "outputs": [],
      "source": [
        "def npchunk_features(sentence, i, history):\n",
        "  word, pos = sentence[i]\n",
        "  return {'pos': pos}\n",
        "\n",
        "chunker = ConsecutiveNPChunker(train_sents)\n",
        "print(chunker.evaluate(test_sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4Oklh_vOtyu"
      },
      "outputs": [],
      "source": [
        "def npchunk_features(sentence, i, history):\n",
        "  word, pos = sentence[i]\n",
        "  if i == 0:\n",
        "    prevword, prevpos = \"<START>\", \"<START>\"\n",
        "  else:\n",
        "    prevword, prevpos = sentence[i-1]\n",
        "  \n",
        "  return {'pos': pos, 'prevpos': prevpos}\n",
        "\n",
        "chunker = ConsecutiveNPChunker(train_sents)\n",
        "print(chunker.evaluate(test_sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKsOw6l_PV-E"
      },
      "outputs": [],
      "source": [
        "def npchunk_features(sentence, i, history):\n",
        "  word, pos = sentence[i]\n",
        "  if i == 0:\n",
        "    prevword, prevpos = \"<START>\", \"<START>\"\n",
        "  else:\n",
        "    prevword, prevpos = sentence[i-1]\n",
        "  return {'pos': pos, 'word': word, 'prevpos': prevpos}\n",
        "\n",
        "chunker = ConsecutiveNPChunker(train_sents)\n",
        "print(chunker.evaluate(test_sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6tfOsGkPxOy"
      },
      "outputs": [],
      "source": [
        "def npchunk_features(sentence, i, history):\n",
        "  word, pos = sentence[i]\n",
        "  if i == 0:\n",
        "    prevword, prevpos = \"<START>\", \"<START>\"\n",
        "  else:\n",
        "    prevword, prevpos = sentence[i-1]\n",
        "  \n",
        "  if i == len(sentence) - 1:\n",
        "    nextword, nextpos = \"<END>\", \"<END>\"\n",
        "  else:\n",
        "    nextword, nextpos = sentence[i+1]\n",
        "  \n",
        "  return {'pos': pos,\n",
        "          'word': word,\n",
        "          'prevpos': prevpos,\n",
        "          'nextpos': nextpos,\n",
        "          'prevpos+pos': \"%s+%s\" % (prevpos, pos),\n",
        "          'pos+nextpos': \"%s+%s\" % (pos, nextpos),\n",
        "          'tags-since-dt': tags_since_dt(sentence, i)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_uuyZfLQkG2"
      },
      "outputs": [],
      "source": [
        "def tags_since_dt(sentence, i):\n",
        "  tags = set()\n",
        "  for word, pos in sentence[:i]:\n",
        "    if pos == 'DT':\n",
        "      tags = set()\n",
        "    else:\n",
        "      tags.add(pos)\n",
        "  \n",
        "  return '+'.join(sorted(tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF5FQ3zSQwqU"
      },
      "outputs": [],
      "source": [
        "chunker = ConsecutiveNPChunker(train_sents)\n",
        "print(chunker.evaluate(test_sents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naEZPRAzQ5CV"
      },
      "source": [
        "## 4. Recursion in Linguistic Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp1Emf-kT9ET"
      },
      "source": [
        "### Building Nested Structure with Cascaded Chunkers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP6ocMb8UO7G",
        "outputId": "3d17aae3-d4cc-4d1d-e64e-18c86fab23b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP Mary/NN)\n",
            "  saw/VBD\n",
            "  (CLAUSE\n",
            "    (NP the/DT cat/NN)\n",
            "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
          ]
        }
      ],
      "source": [
        "grammar = r\"\"\"\n",
        "NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
        "PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
        "VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
        "CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
        "\"\"\"\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
        "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
        "\n",
        "print(cp.parse(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tYuG_JFU8rP",
        "outputId": "476f9a91-0ed8-4ee6-ede0-9df530565110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP John/NNP)\n",
            "  thinks/VBZ\n",
            "  (NP Mary/NN)\n",
            "  saw/VBD\n",
            "  (CLAUSE\n",
            "    (NP the/DT cat/NN)\n",
            "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
          ]
        }
      ],
      "source": [
        "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
        "            (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
        "            (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
        "\n",
        "print(cp.parse(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NizOLd6KVQNA",
        "outputId": "158a1bb6-3174-49c5-af62-50273a901490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP John/NNP)\n",
            "  thinks/VBZ\n",
            "  (CLAUSE\n",
            "    (NP Mary/NN)\n",
            "    (VP\n",
            "      saw/VBD\n",
            "      (CLAUSE\n",
            "        (NP the/DT cat/NN)\n",
            "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
          ]
        }
      ],
      "source": [
        "cp = nltk.RegexpParser(grammar, loop=2)\n",
        "\n",
        "print(cp.parse(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93v-OmD8Visf"
      },
      "source": [
        "### Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfq1O9cqV0lY",
        "outputId": "5389b369-68c4-4dfd-87c8-aeb4cd11d9eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(NP Alice)\n"
          ]
        }
      ],
      "source": [
        "tree1 = nltk.Tree('NP', ['Alice'])\n",
        "print(tree1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKg8JJGqV6mf",
        "outputId": "86e34c4e-e3e1-4dfb-8521-e5c21e7e933c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(NP the rabbit)\n"
          ]
        }
      ],
      "source": [
        "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
        "print(tree2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52fWCpZ0V_RD",
        "outputId": "a25c29ec-3b86-457f-ef67-9637c43378d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S (NP Alice) (VP chased (NP the rabbit)))\n"
          ]
        }
      ],
      "source": [
        "tree3 = nltk.Tree('VP', ['chased', tree2])\n",
        "tree4 = nltk.Tree('S', [tree1, tree3])\n",
        "print(tree4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hTefaj6WIcI",
        "outputId": "7c16c317-d37a-4ce1-c1d3-29b93b1fe3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(VP chased (NP the rabbit))\n"
          ]
        }
      ],
      "source": [
        "print(tree4[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "4S6cpwMhWQyC",
        "outputId": "f3a22312-3729-4e12-e67e-de325ed6ddbb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'VP'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree4[1].label()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzvNxFXCWTmh",
        "outputId": "7226b504-9bf1-4faf-b732-6e5f08d0498a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Alice', 'chased', 'the', 'rabbit']"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree4.leaves()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "NhYKvWzaWVnz",
        "outputId": "ca5235a9-06f4-4d5d-f027-ef24e1ffb1a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'rabbit'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree4[1][1][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMzj1q6RWjZy"
      },
      "source": [
        "### Tree Traversal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH2V0TCpWmSi"
      },
      "outputs": [],
      "source": [
        "def traverse(t):\n",
        "  try:\n",
        "    t.label()\n",
        "  except AttributeError:\n",
        "    print(t, end=' ')\n",
        "  else:\n",
        "    # Now we know that t.node is defined\n",
        "    print('(', t.label(), end=' ')\n",
        "    for child in t:\n",
        "      traverse(child)\n",
        "    print(')', end=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ZVxFSpv1W9es",
        "outputId": "858e9b67-9823-4aee-c44a-973e42867194"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-cd9a578e4c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(S (NP Alice) (VP chased (NP the rabbit)))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node, children)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchildren\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise TypeError(\"%s: Expected a node value and child list \"\n\u001b[0;32m--> 103\u001b[0;31m                                 % type(self).__name__)\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             raise TypeError(\"%s() argument 2 should be a list, not a \"\n",
            "\u001b[0;31mTypeError\u001b[0m: Tree: Expected a node value and child list "
          ]
        }
      ],
      "source": [
        "t = nltk.Tree('(S (NP Alice) (VP chased (NP the rabbit)))')\n",
        "traverse(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb24WrL7XpFH"
      },
      "source": [
        "## 5. Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMYGXOqJZE8M",
        "outputId": "ca1994ed-22da-46a0-98c3-dc082e3a6893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpIAvczcZHaT",
        "outputId": "5d0e5cfc-4d96-4793-914a-d38727785777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (NE U.S./NNP)\n",
            "  is/VBZ\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  few/JJ\n",
            "  industrialized/VBN\n",
            "  nations/NNS\n",
            "  that/WDT\n",
            "  *T*-7/-NONE-\n",
            "  does/VBZ\n",
            "  n't/RB\n",
            "  have/VB\n",
            "  a/DT\n",
            "  higher/JJR\n",
            "  standard/NN\n",
            "  of/IN\n",
            "  regulation/NN\n",
            "  for/IN\n",
            "  the/DT\n",
            "  smooth/JJ\n",
            "  ,/,\n",
            "  needle-like/JJ\n",
            "  fibers/NNS\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  crocidolite/NN\n",
            "  that/WDT\n",
            "  *T*-1/-NONE-\n",
            "  are/VBP\n",
            "  classified/VBN\n",
            "  *-5/-NONE-\n",
            "  as/IN\n",
            "  amphobiles/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (NE Brooke/NNP)\n",
            "  T./NNP\n",
            "  Mossman/NNP\n",
            "  ,/,\n",
            "  a/DT\n",
            "  professor/NN\n",
            "  of/IN\n",
            "  pathlogy/NN\n",
            "  at/IN\n",
            "  the/DT\n",
            "  (NE University/NNP)\n",
            "  of/IN\n",
            "  (NE Vermont/NNP College/NNP)\n",
            "  of/IN\n",
            "  (NE Medicine/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
        "print(nltk.ne_chunk(sent, binary=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVgPOfyRZM6Z",
        "outputId": "a7118e0f-9671-4dfb-8e16-f71c7b68f2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (GPE U.S./NNP)\n",
            "  is/VBZ\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  few/JJ\n",
            "  industrialized/VBN\n",
            "  nations/NNS\n",
            "  that/WDT\n",
            "  *T*-7/-NONE-\n",
            "  does/VBZ\n",
            "  n't/RB\n",
            "  have/VB\n",
            "  a/DT\n",
            "  higher/JJR\n",
            "  standard/NN\n",
            "  of/IN\n",
            "  regulation/NN\n",
            "  for/IN\n",
            "  the/DT\n",
            "  smooth/JJ\n",
            "  ,/,\n",
            "  needle-like/JJ\n",
            "  fibers/NNS\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  crocidolite/NN\n",
            "  that/WDT\n",
            "  *T*-1/-NONE-\n",
            "  are/VBP\n",
            "  classified/VBN\n",
            "  *-5/-NONE-\n",
            "  as/IN\n",
            "  amphobiles/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
            "  ,/,\n",
            "  a/DT\n",
            "  professor/NN\n",
            "  of/IN\n",
            "  pathlogy/NN\n",
            "  at/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION University/NNP)\n",
            "  of/IN\n",
            "  (PERSON Vermont/NNP College/NNP)\n",
            "  of/IN\n",
            "  (GPE Medicine/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "print(nltk.ne_chunk(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEv4iRQcZhMR"
      },
      "source": [
        "## 6. Relation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "NMl2ohHcaGoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434d0575-f9a5-4671-8625-35ea138fd67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/ieer.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "nltk.download('ieer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KF2X6O8Zy5T",
        "outputId": "94d9782f-ad78-44fa-c941-0f7b18aad34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
            "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
            "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
            "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
            "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
            "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
            "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
            "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
            "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
            "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
            "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
            "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
            "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
          ]
        }
      ],
      "source": [
        "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
        "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
        "  for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
        "    print(nltk.sem.rtuple(rel))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('conll2002')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKZSTHnObo4m",
        "outputId": "c8e51988-d087-4cc2-e30a-9b2cfbbf27e6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2002.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "rz8QDe8YaF8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24cb2a7-62be-44c9-f69c-44d073d92c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAN('marco_pantani', 'mercatone_uno')\n",
            "VAN('larmuseau', 'abc_containerline')\n",
            "VAN('horst_köhler', 'imf')\n",
            "VAN('simonet', 'binnenlandse_zaken')\n",
            "VAN('guy_quaden', 'nationale_bank')\n",
            "VAN('de_bauw', 'buitenlandse_zaken')\n",
            "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
            "VAN('rosenfeld', 'abc_containerline')\n",
            "VAN('carlo_gepts', 'vt4')\n",
            "VAN('lone_leth_larsen', 'deens_cultureel_centrum')\n",
            "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n",
            "VAN('jean-louis_peninou', 'international_boundaries_research')\n",
            "VAN('lieven', 'honda')\n",
            "VAN('talal_g_shamoon', 'intertrust_technologies_corporation')\n",
            "VAN('albert_frère', 'tractebel')\n",
            "VAN('robert_spatz', 'okc-beweging')\n",
            "VAN('bart_bode', 'broederlijk_delen')\n",
            "VAN('guido_westerwelle', 'fdp')\n",
            "VAN('martin_bril', 'vrij_nederland')\n",
            "VAN('frank_rijkaard', 'vrij_nederland')\n",
            "VAN('filip', 'telecommunicatie')\n",
            "VAN('maurice_buckmaster', 'special_operations_executive')\n",
            "VAN('mukamba', 'commissie-lumumba')\n",
            "VAN('versnick', 'buitenlandse_zaken')\n",
            "VAN('mukamba', 'miba')\n",
            "VAN('bart_bode', 'broederlijk_delen')\n",
            "VAN('annie_lennox', 'eurythmics')\n",
            "VAN('verheyen', 'duivels')\n",
            "VAN('paul_allen', 'microsoft')\n",
            "VAN('cathy_horyn', 'the_new_york_times')\n",
            "VAN('graydon_carter', 'vanity_fair')\n",
            "VAN('danielle_crittenden', \"women's_quarterly\")\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import conll2002\n",
        "vnv = \"\"\"\n",
        "(\n",
        "  is/V|       # 3rd sing present and\n",
        "  was/V|      # past forms of the verb zijn ('be')\n",
        "  werd/V|     # and also present\n",
        "  wordt/V|    # past of worden ('become')\n",
        ")\n",
        ".*            # followed by anything\n",
        "van/Prep      # followed by van ('of')\n",
        "\"\"\"\n",
        "\n",
        "VAN = re.compile(vnv, re.VERBOSE)\n",
        "for doc in conll2002.chunked_sents('ned.train'):\n",
        "  for rel in nltk.sem.extract_rels('PER', 'ORG', doc,\n",
        "                                   corpus='conll2002', pattern=VAN):\n",
        "    print(nltk.sem.clause(rel, relsym=\"VAN\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import conll2002\n",
        "vnv = \"\"\"\n",
        "(\n",
        "  is/V|       # 3rd sing present and\n",
        "  was/V|      # past forms of the verb zijn ('be')\n",
        "  werd/V|     # and also present\n",
        "  wordt/V|    # past of worden ('become')\n",
        ")\n",
        ".*            # followed by anything\n",
        "van/Prep      # followed by van ('of')\n",
        "\"\"\"\n",
        "\n",
        "VAN = re.compile(vnv, re.VERBOSE)\n",
        "for doc in conll2002.chunked_sents('ned.train'):\n",
        "  for rel in nltk.sem.extract_rels('PER', 'ORG', doc,\n",
        "                                   corpus='conll2002', pattern=VAN):\n",
        "    print(nltk.rtuple(rel, lcon=True, rcon=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ-IlAhMbnUh",
        "outputId": "52e9a2e5-c18b-4d02-d1c9-6e0620ea304b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...'De/Art ploegmaat/N van/Prep')[PER: 'Marco/N Pantani/N'] 'en/Conj kopman/N van/Prep' [ORG: 'Mercatone/N Uno/N']('in/Prep deze/Pron'...\n",
            "...'In/Prep dezelfde/Pron periode/N was/V')[PER: 'Larmuseau/N'] 'ook/Adv lid/N van/Prep de/Art interkabinettengroep/N rond/Prep' [ORG: 'ABC/N Containerline/N'](',/Punc die/Pron specifiek/Adj was/V opgericht/V'...\n",
            "...'Dit/Pron heeft/V')[PER: 'Horst/N Köhler/Conj'] ',/Punc de/Art in/Prep mei/N aangetreden/V topman/N van/Prep het/Art' [ORG: 'IMF/N'](',/Punc gisteren/Adv gezegd/V in/Prep'...\n",
            "...'')[PER: 'Simonet/N'] 'heeft/V de/Art bekommernissen/N overgemaakt/V aan/Prep minister/N van/Prep' [ORG: 'Binnenlandse/N Zaken/N'](''...\n",
            "...'De/Art woorden/N komen/V van/Prep gouverneur/N')[PER: 'Guy/N Quaden/N'] 'van/Prep de/Art' [ORG: 'Nationale/Adj Bank/N'](',/Punc en/Conj ze/Pron werden/V woensdag/N'...\n",
            "...'')[PER: 'De/Art Bauw/N'] '(/Punc 34/Num )/Punc was/V vroeger/Adj adjunct-woordvoerder/N van/Prep het/Art ministerie/N van/Prep' [ORG: 'Buitenlandse/N Zaken/N']('en/Conj is/V momenteel/Adj adviseur/N van/Prep'...\n",
            "...'')[PER: \"Cornet/V d'Elzius/N\"] 'is/V op/Prep dit/Pron ogenblik/N kabinetsadviseur/N van/Prep staatssecretaris/N voor/Prep' [ORG: 'Buitenlandse/N Handel/N'](''...\n",
            "...'doorspelen/V van/Prep vertrouwelijke/Adj documenten/N aan/Prep')[PER: 'Rosenfeld/N'] ',/Punc zoals/Conj verslagen/V van/Prep de/Art interkabinettenwerkgroep/N rond/Prep' [ORG: 'ABC/N Containerline/N'](',/Punc beschouwde/V'...\n",
            "...'')[PER: 'Carlo/N Gepts/N'] 'over/Prep de/Art radio-/N en/Conj televisieambities/N van/Prep' [ORG: 'VT4/Num']('in/Prep'...\n",
            "...'Een/Art gesprek/N met/Prep')[PER: 'Lone/N Leth/N Larsen/N'] ',/Punc directeur/N van/Prep het/Art' [ORG: 'Deens/Adj Cultureel/Adj Centrum/N']('in/Prep'...\n",
            "...'')[PER: 'Johan/N Rottiers/N'] 'is/V informaticacoördinator/N van/Prep het/Art' [ORG: 'Kardinaal/N Van/N Roey/N Instituut/N']('in/Prep'...\n",
            "...'')[PER: 'Jean-Louis/N Peninou/N'] 'van/Prep de/Art' [ORG: 'International/N Boundaries/N Research/N']('Unit/N van/Prep de/Art universteit/N van/Prep'...\n",
            "...'Een/Art collega/N en/Conj')[PER: 'Lieven/N'] 'van/Prep' [ORG: 'Honda/N']('rijden/V met/Prep de/Art'...\n",
            "...'')[PER: 'Talal/N G./N Shamoon/N'] 'van/Prep' [ORG: 'InterTrust/N Technologies/N Corporation/N'](',/Punc een/Art bedrijf/N uit/Prep'...\n",
            "...'ageert/V sinds/Prep')[PER: 'Albert/N Frère/N'] 'in/Prep 1996/Num zijn/Pron participatie/N van/Prep 24,5/Num procent/N in/Prep' [ORG: 'Tractebel/N']('aan/Prep de/Art'...\n",
            "...'Maar/Conj')[PER: 'Robert/N Spatz/N'] ',/Punc het/Art hoofd/N van/Prep de/Art' [ORG: 'OKC-beweging/N']('in/Prep'...\n",
            "...'')[PER: 'Bart/N Bode/N'] 'van/Prep' [ORG: 'Broederlijk/N Delen/N']('en/Conj economieprofessor/N en/Conj'...\n",
            "...'Secretaris-generaal/N')[PER: 'Guido/N Westerwelle/N'] 'van/Prep de/Art liberale/Adj' [ORG: 'FDP/N'](',/Punc die/Pron jarenlang/Adj met/Prep'...\n",
            "...'dat/Conj een/Art verloren/V stukje/N van/Prep')[PER: 'Martin/N Bril/N'] 'in/Prep datzelfde/Pron nummer/N van/Prep' [ORG: 'Vrij/N Nederland/N'](',/Punc waarin/Adv hij/Pron zijn/Pron passie/N'...\n",
            "...'\"/Punc Het/N geheim/N van/N')[PER: 'Frank/N Rijkaard/N'] '\"/Punc ,/Punc prijkt/V deze/Pron week/N op/Prep de/Art cover/N van/Prep' [ORG: 'Vrij/N Nederland/N'](',/Punc en/Conj daarmee/Adv is/V de/Art'...\n",
            "...'samen/Adv met/Prep prins/N')[PER: 'Filip/N'] 'en/Conj minister/N van/Prep' [ORG: 'Telecommunicatie/N'](''...\n",
            "...'was/V de/Art rechterhand/N van/Prep kolonel/N')[PER: 'Maurice/N Buckmaster/N'] ',/Punc directeur/N van/Prep de/Art' [ORG: 'Special/N Operations/N Executive/N'](',/Punc die/Pron naar/Prep verluidt/V ook/Adv'...\n",
            "...'Omdat/Conj')[PER: 'Mukamba/N'] 'een/Art mogelijke/Adj getuige/N is/V van/Prep de/Art' [ORG: 'commissie-Lumumba/N']('--/N hij/Pron zat/V in/Prep januari/N'...\n",
            "...'werd/V vermoord/V --/N ,/Punc vroeg/V')[PER: 'Versnick/N'] 'als/Conj commissievoorzitter/N aan/Prep minister/N van/Prep' [ORG: 'Buitenlandse/N Zaken/N'](''...\n",
            "...'')[PER: 'Mukamba/N'] 'zou/V zijn/Pron invloed/N als/Conj ex-voorzitter/N van/Prep de/Art' [ORG: 'MIBA/N'](',/Punc het/Art'...\n",
            "...'Deze/Pron week/N praten/V')[PER: 'Bart/N Bode/N'] 'van/Prep' [ORG: 'Broederlijk/N Delen/N']('en/Conj economieprofessor/N en/Conj'...\n",
            "...'Door/Prep rugproblemen/N van/Prep zangeres/N')[PER: 'Annie/N Lennox/N'] 'wordt/V het/Art concert/N van/Prep' [ORG: 'Eurythmics/N']('vandaag/Adv in/Prep'...\n",
            "...'')[PER: 'Verheyen/N'] 'vervoegde/V de/Art rangen/N van/Prep de/Art' [ORG: 'Duivels/N']('in/Prep de/Art tweede/Num wedstrijd/N van/Prep'...\n",
            "...'ontsproten/N aan/Prep het/Art brein/N van/Prep')[PER: 'Paul/N Allen/Pron'] ',/Punc mede-oprichter/N van/Prep' [ORG: 'Microsoft/N'](',/Punc kan/V het/Pron niet/Adv anders/Adv'...\n",
            "...'probeerde/V te/Prep maken/V op/Prep')[PER: 'Cathy/N Horyn/N'] 'van/Prep' [ORG: 'The/N New/N York/N Times/N'](',/Punc toen/Conj hij/Pron haar/Pron ontving/V'...\n",
            "...'--/N hoofdredacteur/N')[PER: 'Graydon/N Carter/N'] 'van/Prep' [ORG: 'Vanity/N Fair/N'](',/Punc hoofdredacteur/N'...\n",
            "...'Zo/Adv leren/Adj ons/Pron de/Art dames/N')[PER: 'Danielle/N Crittenden/N'] ',/Punc miljonairsvrouw/N en/Conj redactrice/N van/Prep de/Art neoconservatieve/Adj' [ORG: \"Women's/N Quarterly/N\"](',/Punc'...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "07. Extracting Information from Text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNp0Y+6Ndh6P2MTTiXvp8AM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}